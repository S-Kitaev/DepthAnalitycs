# Stereo depth analytics by S\-Kitaev 

## Introduction

Мною был выбран более сложный вариант решения задачи оценки глубины изображения \- stereo depth estimation\. Последовательность шагов, для решения данной задачи было несколько изменено\. Ввиду ограниченности времени не удалось реализовать все шаги\. 

### Последовательность решения задачи, согласно ТЗ:

1. Погружение в предметную область, сбор статей и литературы, изучение истории решения задачи depth estimation \(как mono, так и stereo\)\.
2. Формирование списка наиболее подходящих работ, более глубокая аналитика, формирование ответа на вопрос SOTA \(State\-Of\-The\-Art\) среди датасетов, а также формирование кандидатов для определения SOTA среди легких и тяжелых сеток\.
3. Поиск, сравнительный анализ \(в том числе на hugging face\) кандидатов для ответа на вопрос SOTA среди моделей, формирование финального заключения среди решений, а также изучение примеров применения решений\.
4. Работа с имеющимся датасетом: просмотр примеров при помощи скриптов в репозитории, локальная загрузка датасета, а также формирование приватного датасета на kaggle для инференса на тяжелой модели и дальнейшего обучения легкой модели\.
5. Написание блокнота со скриптом для \(pseudo\-labeling\) изображений при помощи тяжелой модели, формирование отчета инференса, сохранение результатов примеров глубины на изображениях\.
6. Написание блокнота со скриптом обучения легкой модели, использование советов, описанных в ТЗ, демонстрация результатов экспериментов
7. Перенос легкой модели в репозиторий, написание скриптов для быстрого инференса модели, проверка быстродействия\.
8. По итогам работы формирование предположений того, как можно оценить per\-pixel confidence работы модели\.

##  1\. Погружение в предметную область

Ввиду ограниченности времени, и необходимости погружения в новую предметную область, пришлось действовать радикально\. На ресурсах: 

1. [Hugging face papers \(бывш\. papers with code\)](https://huggingface.co/papers/)
2. [SemanticScholar](https://www.semanticscholar.org/)
3. [alphaXiv](https://www.alphaxiv.org/)
4. [Github автора задания](https://github.com/IlyaInd)

Было выделено несколько видов статей:

Глубокие статьи с обзором предметной области с большим количеством цитирований, и просто по теме с большим количеством цитирований:

1. [Efficient Deep Learning for Stereo Matching](https://www.cs.toronto.edu/~urtasun/publications/luo_etal_cvpr16.pdf)
2. [A Surwey on Deep Stereo Matching in the Twenties](https://arxiv.org/abs/2407.07816)

Статьи с относительно большим количеством цитирований, но при этом написанные за последний год \(самые свежие\), например:

1. [Stereo Anywhere: Robust Zero\-Shot Deep Stereo Matching Even Where Either Stereo or Mono Fail](https://arxiv.org/abs/2412.04472)
2. [MVSAnywhere: Zero\-Shot Multi\-View Stereo](https://arxiv.org/abs/2503.22430)

Дополнительные статьи, которые могут помочь в решении данного задания:

1. [Статья, которая \(возможно\) связана с автором тестового задания](https://www.arxiv.org/abs/2509.21388)
2. [Depth Anything V2](https://arxiv.org/abs/2406.09414) \(по количеству цитирований, новизне,  скачиваний с hugging face \- SOTA для задач mono depth estimation\)

##  2\. Аналитика предметной области

[Efficient Deep Learning for Stereo Matching](https://www.cs.toronto.edu/~urtasun/publications/luo_etal_cvpr16.pdf) \(2016 г\.\):

Ранее типичным подход для обработки стереопары были поступление 2х изображений на вход нейросети, отдельная обработка каждого изображения своей CNN, конкатенация выходов левой и правой CNN и затем конкатенация\. Сети такой архитектуры были медленные и не точные\. В данной работе предложено не конкатенировать результат, а создавать product layer \- скалярное произведение левого и правого CNN, что позволило как сократить время на обработку пары &lt; 1сек, так и увеличить точность, также проводился анализ 

Решаемая задача \- многоклассовая классификация \(каждому пикселю сопоставляется 1 из 128&#x2F;256 классов\), 

функция потерь \- CrossEntropyLoss

Датасеты \- KITTI 2012, KITTI 2015

[A Surwey on Deep Stereo Matching in the Twenties](https://arxiv.org/abs/2407.07816)\(2024 г\.\):

Сразу перейдем к ключевой статье нашего обзора, она охватывает 4 года исследований \(2020\-2024\) в области stereo depth estimation\. В последние годы произошло несколько ключевых изменений, которые изменили вектор развития области\.

### 1\) CNN\-basedCostVolumeAggregation

Одна из финальных точек развития CNN архитектур, на основе базовых принципов, пример которых приведен [Efficient Deep Learning for Stereo Matching](https://www.cs.toronto.edu/~urtasun/publications/luo_etal_cvpr16.pdf) и путем оптимизации архитектуры к 2020 году была достигнута лучшая производительность и точность сети\. Один из ключевых примеров \- [AANet](https://github.com/haofeixu/aanet)\. На данный момент это считается устаревшей архитектурой\.

### 2\)  Iterative Optimization\-based Architectures 

первая из двух революций в области stereo matching\. Использование мета\-нейросети для формирования архитектуры рабочей нейросети, чтобы она выдавала наилучшее качество\. Является сверхтребовательным подходом, и лучшей адаптацией данного подхода на момент 2020\-2022 год являлась сеть [RAFT\-Stereo](https://github.com/princeton-vl/RAFT-Stereo), которая объединила optical flow подход и оператора обновления на основе сверточных рекуррентных блоков \(ConvGRU\)\. Эволюция данного подхода отражена в нейросетях: 

- [CREStereo](https://github.com/megvii-research/CREStereo) \(приведено в качестве устаревшей сети в ТЗ задания\) \(2022\)

представляет иерархическую сеть с рекуррентным уточнением и адаптивным групповым корреляционным слоем \(AGCL\)\. Вместо того чтобы искать соответствия для всего изображения целиком, сеть разбивает его на группы и ищет соответствия для каждой группы независимо\. Это делает сопоставление более точным и устойчивым к сбоям в сложных областях\.

- [CREStereo\+\+](https://scholar.google.com.hk/citations?view_op=view_citation&hl=en&user=OefyYf0AAAAJ&citation_for_view=OefyYf0AAAAJ:M05iB0D1s5AC)\(2023\)

Сеть сначала оценивает, *насколько ненадежным* является сопоставление в данной области\. Затем, в этих ненадежных областях она автоматически расширяет зону поиска соответствий

- [IGEV\-Stereo](https://github.com/gangweiX/IGEV)\(2023\)

Строит 2 Cost\-volume: легковесный \(как в RAFT\) для захвата тонких деталей и регуляризованный 3D свертками \(как в старых подходах\) для понимания общей геометрии сцены\. Затем они объединяются\.

- [ICGNet](https://github.com/DFSDDDDD1199/ICGNet)\(2024\)

Использование ключевых точек для mono и stereo согласованности\.

- [IGEV\+\+](https://github.com/gangweiX/IGEV)\(2025\)

Вместо одного подхода для всех областей изображения, сеть строит три отдельных &quot;геометрических объема&quot; для разных диапазонов диспарантности \(малый, средний, большой\) и затем объединяет результаты \(смотрит на разные масштабы\)

### 3\) Vision Transformer\-based Architectures

Вторая и более известная революция в области как нейросетей, так CV в целом и в частности в области stereo matching\. Переносит идею &quot;внимания&quot; на стерео\. Вместо cost volume использует самовнимание и кросс\-внимание между пикселями левого и правого изображений\. Ключевые представители:

- [STTR](https://github.com/mli0603/stereo-transformer)\(2021\-2022\)

Сеть объединяет традиционный CNN и модуль захвата отношений на больших расстояниях \- Transformer\. Включает self\-attention, cross\-attention, and positional encoding schemes\. Результаты сравнимы с другими архитектурами на момент 2021 года\.

- [GMstereo](https://haofeixu.github.io/unimatch/)\(2023\)

Модель объединяет подходы трансформера и optical flow\. Единая модель для трёх задач: оптического потока, стерео\-сопоставления и оценки глубины по двум изображениям с известными позами камер\. 

Эффективность тяжелых моделей до 2024 года на датасете KITTI 2015 представлена на рисунке ниже \(CREStereo на 13 месте\)

![\[https://miro.com/app/board/uXjVM0dLqLY=/?moveToWidget=3458764645695220246&cot=14\](https://miro.com/app/board/uXjVM0dLqLY=/?moveToWidget=3458764645695220246&cot=14)](img/image1.png)

Эффективность легких моделей до 2024 года на датасете KITTI 2015 представлена на рисунке ниже \(BGNet на 6 месте\)

![\[https://miro.com/app/board/uXjVM0dLqLY=/?moveToWidget=3458764645695298677&cot=14\](https://miro.com/app/board/uXjVM0dLqLY=/?moveToWidget=3458764645695298677&cot=14)](img/image2.png)

## Тяжелая SOTA модель

Анализ статей авторов тяжелых моделей показал, что кросс\-упоминание моделей друг друга больше всего у моделей CREStereo, RAFT\-Stereo, GMstereo, а также у двух из новейших моделей \- HART \(нет в данном списке, так как была релизнута позже\) и у модели **Selective\-Stereo \(Selective\-IGEV\)**, которая и будет взята в качестве тяжелой SOTA модели\. 

Ссылка на репозиторий модели \- **[Selective\-Stereo \(Selective\-IGEV\)](https://github.com/Windsrain/Selective-Stereo/tree/main)**

Логика \- если на современную модель \(2024\-2025\) много кто ссылается, следовательно ее рассматривают как SOTA модель\. Selective\-IGEV \- это продолжение развития модели IGEV, о которой упоминалось ранее

## Легкая SOTA модель

Анализ статей авторов легких моделей показывает что из современных моделей самые частые упоминания у PCVNet, AANet\+\+, CoEX, Temporal Stereo, но в качестве SOTA подхода, было принято решение выбрать [RT\-IGEV](https://github.com/gangweix/IGEV-plusplus?tab=readme-ov-file) модель\.

## SOTA датасеты

В абсолютно всех изученных работах были приведены одни и те же базовые датасеты, по которым сравнивалась робастность моделей это KITTY 2012, KITTY 2015, и датасет из сгенерированных данных \- Scene Flow\. В современных моделях также часто упоминаются датасеты MiddleBury и ETH3D\. Для нашей задачи считаю наилучшим датасетом будет добавить MiddleBury датасет, так как данные в нем максимально схожи с выборкой InStereo2K, на базе которой был составлен датасет к данной задаче

## ИТОГ

SOTA датасет \- **[MiddleBury](https://vision.middlebury.edu/stereo/data/)**

SOTA тяжелая модель \- **[Selective\-Stereo \(Selective\-IGEV\)](https://github.com/Windsrain/Selective-Stereo/tree/main)**

SOTA легкая модель \- **[RT\-IGEV](https://github.com/gangweix/IGEV-plusplus?tab=readme-ov-file)**